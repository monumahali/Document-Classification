{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import collections\n",
    "import math\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words: 202528\n",
      "doc words: ['ad', 'sales', 'boost', 'time', 'warner', 'profit', 'quarterly', 'profits', 'at', 'us']\n"
     ]
    }
   ],
   "source": [
    "topics = ['business','entertainment','politics','sport','tech']\n",
    "\n",
    "def read_data(topics):\n",
    "    \n",
    "    num_of_docs_to_read = 100\n",
    "    doc_words = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        for doc in range(1,num_of_docs_to_read + 1):\n",
    "\n",
    "            with open(os.path.join('bbc', topic, format(doc, '03d') + '.txt')) as f:\n",
    "                document = f.read()\n",
    "                file = document.lower()\n",
    "                file = nltk.word_tokenize(file)\n",
    "                doc_words.extend(file)\n",
    "\n",
    "    return doc_words\n",
    "\n",
    "def read_test_data(topics):\n",
    "    \n",
    "    num_of_docs_to_read = 100\n",
    "    test_doc_words_dict = {}\n",
    "    \n",
    "    for topic in topics:\n",
    "        for doc in np.random.randint(1,num_of_docs_to_read,(10)).tolist():\n",
    "\n",
    "            with open(os.path.join('bbc', topic, format(doc, '03d') + '.txt')) as f:\n",
    "                document = f.read()\n",
    "                file = document.lower()\n",
    "                file = nltk.word_tokenize(file)\n",
    "                test_doc_words_dict[topic + '-' + str(doc)] = file\n",
    "                \n",
    "    return test_doc_words_dict\n",
    "\n",
    "doc_words = read_data(topics)\n",
    "test_doc_words_dict = read_test_data(topics)\n",
    "\n",
    "print('no. of words:', len(doc_words))\n",
    "print('doc words:', doc_words[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary ['unparalleled', 'aimed', 'italians', 'drops', '25m', 'contesting', 'urban-based', '7.35.', 'breaches', 'sanguine']\n",
      "reverse dictionary [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "most common words: [['UNK', -1], ('the', 11245), ('.', 8249), (',', 6896), ('to', 5203)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000\n",
    "\n",
    "def create_dict(doc_words):\n",
    "    global vocab_size\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(doc_words).most_common(vocab_size - 1))\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    rev_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    return dictionary, rev_dictionary, count\n",
    "\n",
    "dictionary, rev_dictionary, count = create_dict(doc_words)\n",
    "\n",
    "print('dictionary', list(dictionary)[:10])\n",
    "print('reverse dictionary', list(rev_dictionary)[:10])\n",
    "print('most common words:', count[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words: 202528\n",
      "doc int: [4724, 155, 746, 86, 1571, 837, 3164, 580, 26, 58]\n"
     ]
    }
   ],
   "source": [
    "def str_to_int(doc_words, dictionary):\n",
    "    doc_int = []\n",
    "\n",
    "    for word in doc_words:\n",
    "        if word in dictionary:\n",
    "            doc_int.append(dictionary[word])\n",
    "        else:\n",
    "            doc_int.append(dictionary['UNK'])\n",
    "\n",
    "    return doc_int\n",
    "\n",
    "\n",
    "\n",
    "doc_int = str_to_int(doc_words, dictionary)\n",
    "\n",
    "print('no. of words:', len(doc_int))\n",
    "print('doc int:', doc_int[:10])\n",
    "\n",
    "test_doc_int = {}\n",
    "\n",
    "for doc, words in test_doc_words_dict.items():\n",
    "    test_doc_int[doc] = str_to_int(test_doc_words_dict[doc], dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [[4724, 155, 86, 1571], [155, 746, 1571, 837], [746, 86, 837, 3164], [86, 1571, 3164, 580], [1571, 837, 580, 26], [837, 3164, 26, 58], [3164, 580, 58, 438], [580, 26, 438, 1082], [26, 58, 1082, 3065], [58, 438, 3065, 2651]]\n",
      "\n",
      "labels: [746, 86, 1571, 837, 3164, 580, 26, 58, 438, 1082]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "window_size = 2\n",
    "\n",
    "def gen_data_labels(doc_int, window_size):\n",
    "    global data\n",
    "    global labels\n",
    "    \n",
    "    for i in range(len(doc_int)):\n",
    "    \n",
    "        if i < len(doc_int) - window_size:\n",
    "\n",
    "            [data.append(doc_int[i : i + window_size] + doc_int[i + window_size + 1 : i + 2*window_size + 1])]\n",
    "            labels.append(doc_int[i + window_size])\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "data, labels = gen_data_labels(doc_int, window_size)\n",
    "\n",
    "print('data:', data[:10])\n",
    "print('\\nlabels:', labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202526"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating batches for CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch data: \n",
      "\n",
      "[[ 4724   155    86  1571]\n",
      " [  155   746  1571   837]\n",
      " [  746    86   837  3164]\n",
      " [   86  1571  3164   580]\n",
      " [ 1571   837   580    26]\n",
      " [  837  3164    26    58]\n",
      " [ 3164   580    58   438]\n",
      " [  580    26   438  1082]\n",
      " [   26    58  1082  3065]\n",
      " [   58   438  3065  2651]\n",
      " [  438  1082  2651 11177]\n",
      " [ 1082  3065 11177    49]\n",
      " [ 3065  2651    49     4]\n",
      " [ 2651 11177     4    84]\n",
      " [11177    49    84  7018]\n",
      " [   49     4  7018    51]\n",
      " [    4    84    51  9434]\n",
      " [   84  7018  9434    50]\n",
      " [ 7018    51    50    11]\n",
      " [   51  9434    11     1]\n",
      " [ 9434    50     1   120]\n",
      " [   50    11   120   197]\n",
      " [   11     1   197     4]\n",
      " [    1   120     4   337]\n",
      " [  120   197   337     3]\n",
      " [  197     4     3    31]\n",
      " [    4   337    31    84]\n",
      " [  337     3    84 11308]\n",
      " [    3    31 11308  6626]\n",
      " [   31    84  6626     2]\n",
      " [   84 11308     2     1]\n",
      " [11308  6626     1   136]\n",
      " [ 6626     2   136     3]\n",
      " [    2     1     3    34]\n",
      " [    1   136    34    13]\n",
      " [  136     3    13    82]\n",
      " [    3    34    82    65]\n",
      " [   34    13    65     5]\n",
      " [   13    82     5     1]\n",
      " [   82    65     1   443]\n",
      " [   65     5   443  1042]\n",
      " [    5     1  1042     6]\n",
      " [    1   443     6   602]\n",
      " [  443  1042   602     3]\n",
      " [ 1042     6     3  9875]\n",
      " [    6   602  9875    31]\n",
      " [  602     3    31   155]\n",
      " [    3  9875   155     5]\n",
      " [ 9875    31     5  1771]\n",
      " [   31   155  1771   240]\n",
      " [  155     5   240  2372]\n",
      " [    5  1771  2372     7]\n",
      " [ 1771   240     7   777]\n",
      " [  240  2372   777  5721]\n",
      " [ 2372     7  5721   155]\n",
      " [    7   777   155     2]\n",
      " [  777  5721     2  3065]\n",
      " [ 5721   155  3065    16]\n",
      " [  155     2    16   640]\n",
      " [    2  3065   640   518]\n",
      " [ 3065    16   518   155]\n",
      " [   16   640   155   666]\n",
      " [  640   518   666   809]\n",
      " [  518   155   809    49]]\n",
      " \n",
      "batch labels: \n",
      "\n",
      "[[  746]\n",
      " [   86]\n",
      " [ 1571]\n",
      " [  837]\n",
      " [ 3164]\n",
      " [  580]\n",
      " [   26]\n",
      " [   58]\n",
      " [  438]\n",
      " [ 1082]\n",
      " [ 3065]\n",
      " [ 2651]\n",
      " [11177]\n",
      " [   49]\n",
      " [    4]\n",
      " [   84]\n",
      " [ 7018]\n",
      " [   51]\n",
      " [ 9434]\n",
      " [   50]\n",
      " [   11]\n",
      " [    1]\n",
      " [  120]\n",
      " [  197]\n",
      " [    4]\n",
      " [  337]\n",
      " [    3]\n",
      " [   31]\n",
      " [   84]\n",
      " [11308]\n",
      " [ 6626]\n",
      " [    2]\n",
      " [    1]\n",
      " [  136]\n",
      " [    3]\n",
      " [   34]\n",
      " [   13]\n",
      " [   82]\n",
      " [   65]\n",
      " [    5]\n",
      " [    1]\n",
      " [  443]\n",
      " [ 1042]\n",
      " [    6]\n",
      " [  602]\n",
      " [    3]\n",
      " [ 9875]\n",
      " [   31]\n",
      " [  155]\n",
      " [    5]\n",
      " [ 1771]\n",
      " [  240]\n",
      " [ 2372]\n",
      " [    7]\n",
      " [  777]\n",
      " [ 5721]\n",
      " [  155]\n",
      " [    2]\n",
      " [ 3065]\n",
      " [   16]\n",
      " [  640]\n",
      " [  518]\n",
      " [  155]\n",
      " [  666]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def next_batch(batch_no, batch_size):\n",
    "    \n",
    "    batch_data = data[batch_no*batch_size : (batch_no * batch_size) + batch_size]\n",
    "    batch_data = np.array(batch_data)\n",
    "    \n",
    "    batch_labels = labels[batch_no*batch_size : (batch_no * batch_size) + batch_size]\n",
    "    batch_labels = np.array(batch_labels)\n",
    "    batch_labels = np.reshape(batch_labels, (batch_size, 1))\n",
    "    \n",
    "    return batch_data, batch_labels\n",
    "\n",
    "batch_data, batch_labels = next_batch(0, batch_size)\n",
    "\n",
    "print('batch data: \\n\\n{}\\n \\nbatch labels: \\n\\n{}'.format(batch_data, batch_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_index = 0\n",
    "\n",
    "def generate_test_batch_words(data, batch_size, num_test_steps):\n",
    "    global test_data_index\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size,), dtype=np.int32)\n",
    "    \n",
    "    for bi in range(batch_size):\n",
    "        batch[bi] = data[test_data_index + num_test_steps]\n",
    "        test_data_index = (test_data_index + 1) % batch_size\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 128 \n",
    "window_size = 2\n",
    "num_sampled = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/p and O/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_data = tf.placeholder(tf.int32, shape = [batch_size, 2 * window_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
    "\n",
    "test_labels = tf.placeholder(tf.int32, shape = [batch_size], name = 'test_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0, dtype = tf.float32))\n",
    "\n",
    "softmax_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev = 1.0 / math.sqrt(embedding_size)))\n",
    "softmax_biases = tf.Variable(tf.zeros([vocab_size]), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked embedding size: [64, 128, 4]\n",
      "Reduced mean embedding size: [64, 128]\n"
     ]
    }
   ],
   "source": [
    "mean_batch_embeddings = tf.reduce_mean(tf.nn.embedding_lookup(embeddings, test_labels), axis = 0)\n",
    "\n",
    "stacked_embeddings = None\n",
    "\n",
    "for i in range(2*window_size):\n",
    "    embedding_i = tf.nn.embedding_lookup(embeddings, train_data[:, i])\n",
    "    x_size, y_size = embedding_i.get_shape().as_list()\n",
    "    \n",
    "    if stacked_embeddings is None:\n",
    "        stacked_embeddings = tf.reshape(embedding_i, [x_size, y_size, 1])\n",
    "    else:\n",
    "        stacked_embeddings = tf.concat(axis = 2, values = [stacked_embeddings, tf.reshape(embedding_i, [x_size, y_size, 1])])\n",
    "\n",
    "        \n",
    "print(\"Stacked embedding size: %s\"%stacked_embeddings.get_shape().as_list())\n",
    "\n",
    "mean_embeddings = tf.reduce_mean(stacked_embeddings, axis = 2, keepdims = False)\n",
    "\n",
    "print(\"Reduced mean embedding size: %s\"%mean_embeddings.get_shape().as_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/monu/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights = softmax_weights,\n",
    "                                                 biases = softmax_biases,\n",
    "                                                 inputs = mean_embeddings,\n",
    "                                                 labels = train_labels,\n",
    "                                                 num_sampled = num_sampled,\n",
    "                                                 num_classes = vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CBOW on Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 5.838185\n",
      "Average loss at step 200: 4.332036\n",
      "Average loss at step 400: 3.547339\n",
      "Average loss at step 600: 3.401832\n",
      "Average loss at step 800: 3.379141\n",
      "Average loss at step 1000: 3.123720\n",
      "Average loss at step 1200: 3.221186\n",
      "Average loss at step 1400: 3.218886\n",
      "Average loss at step 1600: 3.086449\n",
      "Average loss at step 1800: 2.972738\n",
      "Average loss at step 2000: 3.150804\n",
      "Average loss at step 2200: 2.821532\n",
      "Average loss at step 2400: 2.895416\n",
      "Average loss at step 2600: 3.206407\n",
      "Average loss at step 2800: 3.090301\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "cbow_loss = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        batch_data, batch_labels = next_batch(step, batch_size)\n",
    "    \n",
    "        feed_dict = {train_data: batch_data, train_labels: batch_labels}\n",
    "        \n",
    "        _, l = sess.run([optimizer, loss], feed_dict = feed_dict)\n",
    "        avg_loss += l\n",
    "        \n",
    "        if (step) % 200 == 0:\n",
    "            if step > 0:\n",
    "                avg_loss = avg_loss / 200\n",
    "            print('Average loss at step %d: %f' % (step, avg_loss))\n",
    "            \n",
    "            cbow_loss.append(avg_loss)\n",
    "    \n",
    "    # Computing document embeddings by averaging word embeddings\n",
    "    document_embeddings = {}\n",
    "    num_test_steps = 5\n",
    "    \n",
    "    for doc, words in test_doc_int.items():\n",
    "        \n",
    "        test_data_index = 0\n",
    "        topic_mean_batch_embeddings = np.empty((num_test_steps,embedding_size),dtype=np.float32)\n",
    "        \n",
    "        for step in range(num_test_steps):\n",
    "            test_batch_labels = generate_test_batch_words(test_doc_int[doc],batch_size, step)\n",
    "            batch_mean = sess.run(mean_batch_embeddings,feed_dict={test_labels:test_batch_labels})\n",
    "            \n",
    "            topic_mean_batch_embeddings[step, :] = batch_mean\n",
    "        document_embeddings[doc] = np.mean(topic_mean_batch_embeddings, axis = 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents in Cluster  0\n",
      "\t ['sport-23', 'sport-93', 'politics-60', 'sport-2', 'sport-75', 'politics-6']\n",
      "\n",
      "Documents in Cluster  1\n",
      "\t ['tech-17', 'tech-44', 'entertainment-74', 'sport-89', 'sport-47', 'tech-52', 'politics-19', 'politics-22', 'tech-79', 'tech-72', 'tech-9', 'sport-24', 'entertainment-55', 'business-96', 'politics-39', 'tech-27', 'business-41', 'politics-67']\n",
      "\n",
      "Documents in Cluster  2\n",
      "\t ['sport-66', 'business-78', 'entertainment-72', 'politics-43', 'business-77', 'business-28', 'tech-84', 'entertainment-26', 'entertainment-48', 'entertainment-54']\n",
      "\n",
      "Documents in Cluster  3\n",
      "\t ['business-94', 'entertainment-85', 'entertainment-33', 'tech-56', 'entertainment-81', 'entertainment-2', 'sport-64', 'business-11']\n",
      "\n",
      "Documents in Cluster  4\n",
      "\t ['business-74', 'business-49', 'politics-71', 'business-98', 'politics-32']\n"
     ]
    }
   ],
   "source": [
    "kmeans = kmeans = KMeans(n_clusters=5, random_state=0, max_iter = 10000)\n",
    "kmeans.fit(np.array(list(document_embeddings.values())))\n",
    "\n",
    "document_classes = {}\n",
    "\n",
    "for inp, lbl in zip(list(document_embeddings.keys()), kmeans.labels_):\n",
    "    if lbl not in document_classes:\n",
    "        document_classes[lbl] = [inp]\n",
    "    else:\n",
    "        document_classes[lbl].append(inp)\n",
    "\n",
    "for k,v in document_classes.items():    \n",
    "    print('\\nDocuments in Cluster ',k)\n",
    "    print('\\t',v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
