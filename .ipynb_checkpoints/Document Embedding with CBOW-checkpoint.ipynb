{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words: 202528\n",
      "doc words: ['ad', 'sales', 'boost', 'time', 'warner', 'profit', 'quarterly', 'profits', 'at', 'us']\n"
     ]
    }
   ],
   "source": [
    "topics = ['business','entertainment','politics','sport','tech']\n",
    "\n",
    "def read_data(topics):\n",
    "    \n",
    "    num_of_docs_to_read = 100\n",
    "    doc_words = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        for doc in range(1,num_of_docs_to_read + 1):\n",
    "\n",
    "            with open(os.path.join('bbc', topic, format(doc, '03d') + '.txt')) as f:\n",
    "                document = f.read()\n",
    "                file = document.lower()\n",
    "                file = nltk.word_tokenize(file)\n",
    "                doc_words.extend(file)\n",
    "\n",
    "    return doc_words\n",
    "\n",
    "def read_test_data(topics):\n",
    "    \n",
    "    num_of_docs_to_read = 100\n",
    "    test_doc_words_dict = {}\n",
    "    \n",
    "    for topic in topics:\n",
    "        for doc in np.random.randint(1,num_of_docs_to_read,(10)).tolist():\n",
    "\n",
    "            with open(os.path.join('bbc', topic, format(doc, '03d') + '.txt')) as f:\n",
    "                document = f.read()\n",
    "                file = document.lower()\n",
    "                file = nltk.word_tokenize(file)\n",
    "                test_doc_words_dict[topic + '-' + str(doc)] = file\n",
    "                \n",
    "    return test_doc_words_dict\n",
    "\n",
    "doc_words = read_data(topics)\n",
    "test_doc_words_dict = read_test_data(topics)\n",
    "\n",
    "print('no. of words:', len(doc_words))\n",
    "print('doc words:', doc_words[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary ['deadwood', 'barley', 'gorgeous', 'judge-led', 'send', 'finland', '3.25', 'nor', 'tracker', 'dedicated']\n",
      "reverse dictionary [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "most common words: [['UNK', -1], ('the', 11245), ('.', 8249), (',', 6896), ('to', 5203)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000\n",
    "\n",
    "def create_dict(doc_words):\n",
    "    global vocab_size\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(doc_words).most_common(vocab_size - 1))\n",
    "\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    rev_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "    return dictionary, rev_dictionary, count\n",
    "\n",
    "dictionary, rev_dictionary, count = create_dict(doc_words)\n",
    "\n",
    "print('dictionary', list(dictionary)[:10])\n",
    "print('reverse dictionary', list(rev_dictionary)[:10])\n",
    "print('most common words:', count[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of words: 202528\n",
      "doc int: [4762, 155, 748, 86, 1559, 818, 3133, 580, 26, 58]\n"
     ]
    }
   ],
   "source": [
    "def str_to_int(doc_words, dictionary):\n",
    "    doc_int = []\n",
    "\n",
    "    for word in doc_words:\n",
    "        if word in dictionary:\n",
    "            doc_int.append(dictionary[word])\n",
    "        else:\n",
    "            doc_int.append(dictionary['UNK'])\n",
    "\n",
    "    return doc_int\n",
    "\n",
    "\n",
    "\n",
    "doc_int = str_to_int(doc_words, dictionary)\n",
    "\n",
    "print('no. of words:', len(doc_int))\n",
    "print('doc int:', doc_int[:10])\n",
    "\n",
    "test_doc_int = {}\n",
    "\n",
    "for doc, words in test_doc_words_dict.items():\n",
    "    test_doc_int[doc] = str_to_int(test_doc_words_dict[doc], dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [[4762, 155, 86, 1559], [155, 748, 1559, 818], [748, 86, 818, 3133], [86, 1559, 3133, 580], [1559, 818, 580, 26], [818, 3133, 26, 58], [3133, 580, 58, 438], [580, 26, 438, 1071], [26, 58, 1071, 2966], [58, 438, 2966, 2828]]\n",
      "\n",
      "labels: [748, 86, 1559, 818, 3133, 580, 26, 58, 438, 1071]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "window_size = 2\n",
    "\n",
    "def gen_data_labels(doc_int, window_size):\n",
    "    global data\n",
    "    global labels\n",
    "    \n",
    "    for i in range(len(doc_int)):\n",
    "    \n",
    "        if i < len(doc_int) - window_size:\n",
    "\n",
    "            [data.append(doc_int[i : i + window_size] + doc_int[i + window_size + 1 : i + 2*window_size + 1])]\n",
    "            labels.append(doc_int[i + window_size])\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "data, labels = gen_data_labels(doc_int, window_size)\n",
    "\n",
    "print('data:', data[:10])\n",
    "print('\\nlabels:', labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202526"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating batches for CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch data: \n",
      "\n",
      "[[ 4762   155    86  1559]\n",
      " [  155   748  1559   818]\n",
      " [  748    86   818  3133]\n",
      " [   86  1559  3133   580]\n",
      " [ 1559   818   580    26]\n",
      " [  818  3133    26    58]\n",
      " [ 3133   580    58   438]\n",
      " [  580    26   438  1071]\n",
      " [   26    58  1071  2966]\n",
      " [   58   438  2966  2828]\n",
      " [  438  1071  2828 10144]\n",
      " [ 1071  2966 10144    49]\n",
      " [ 2966  2828    49     4]\n",
      " [ 2828 10144     4    84]\n",
      " [10144    49    84  7648]\n",
      " [   49     4  7648    51]\n",
      " [    4    84    51 13735]\n",
      " [   84  7648 13735    50]\n",
      " [ 7648    51    50    11]\n",
      " [   51 13735    11     1]\n",
      " [13735    50     1   122]\n",
      " [   50    11   122   196]\n",
      " [   11     1   196     4]\n",
      " [    1   122     4   337]\n",
      " [  122   196   337     3]\n",
      " [  196     4     3    31]\n",
      " [    4   337    31    84]\n",
      " [  337     3    84 13344]\n",
      " [    3    31 13344  6522]\n",
      " [   31    84  6522     2]\n",
      " [   84 13344     2     1]\n",
      " [13344  6522     1   137]\n",
      " [ 6522     2   137     3]\n",
      " [    2     1     3    34]\n",
      " [    1   137    34    13]\n",
      " [  137     3    13    82]\n",
      " [    3    34    82    65]\n",
      " [   34    13    65     5]\n",
      " [   13    82     5     1]\n",
      " [   82    65     1   445]\n",
      " [   65     5   445  1006]\n",
      " [    5     1  1006     6]\n",
      " [    1   445     6   602]\n",
      " [  445  1006   602     3]\n",
      " [ 1006     6     3 14185]\n",
      " [    6   602 14185    31]\n",
      " [  602     3    31   155]\n",
      " [    3 14185   155     5]\n",
      " [14185    31     5  1793]\n",
      " [   31   155  1793   243]\n",
      " [  155     5   243  2211]\n",
      " [    5  1793  2211     7]\n",
      " [ 1793   243     7   770]\n",
      " [  243  2211   770  5869]\n",
      " [ 2211     7  5869   155]\n",
      " [    7   770   155     2]\n",
      " [  770  5869     2  2966]\n",
      " [ 5869   155  2966    16]\n",
      " [  155     2    16   630]\n",
      " [    2  2966   630   515]\n",
      " [ 2966    16   515   155]\n",
      " [   16   630   155   665]\n",
      " [  630   515   665   822]\n",
      " [  515   155   822    49]]\n",
      " \n",
      "batch labels: \n",
      "\n",
      "[[  748]\n",
      " [   86]\n",
      " [ 1559]\n",
      " [  818]\n",
      " [ 3133]\n",
      " [  580]\n",
      " [   26]\n",
      " [   58]\n",
      " [  438]\n",
      " [ 1071]\n",
      " [ 2966]\n",
      " [ 2828]\n",
      " [10144]\n",
      " [   49]\n",
      " [    4]\n",
      " [   84]\n",
      " [ 7648]\n",
      " [   51]\n",
      " [13735]\n",
      " [   50]\n",
      " [   11]\n",
      " [    1]\n",
      " [  122]\n",
      " [  196]\n",
      " [    4]\n",
      " [  337]\n",
      " [    3]\n",
      " [   31]\n",
      " [   84]\n",
      " [13344]\n",
      " [ 6522]\n",
      " [    2]\n",
      " [    1]\n",
      " [  137]\n",
      " [    3]\n",
      " [   34]\n",
      " [   13]\n",
      " [   82]\n",
      " [   65]\n",
      " [    5]\n",
      " [    1]\n",
      " [  445]\n",
      " [ 1006]\n",
      " [    6]\n",
      " [  602]\n",
      " [    3]\n",
      " [14185]\n",
      " [   31]\n",
      " [  155]\n",
      " [    5]\n",
      " [ 1793]\n",
      " [  243]\n",
      " [ 2211]\n",
      " [    7]\n",
      " [  770]\n",
      " [ 5869]\n",
      " [  155]\n",
      " [    2]\n",
      " [ 2966]\n",
      " [   16]\n",
      " [  630]\n",
      " [  515]\n",
      " [  155]\n",
      " [  665]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def next_batch(batch_no, batch_size):\n",
    "    \n",
    "    batch_data = data[batch_no*batch_size : (batch_no * batch_size) + batch_size]\n",
    "    batch_data = np.array(batch_data)\n",
    "    \n",
    "    batch_labels = labels[batch_no*batch_size : (batch_no * batch_size) + batch_size]\n",
    "    batch_labels = np.array(batch_labels)\n",
    "    batch_labels = np.reshape(batch_labels, (batch_size, 1))\n",
    "    \n",
    "    return batch_data, batch_labels\n",
    "\n",
    "batch_data, batch_labels = next_batch(0, batch_size)\n",
    "\n",
    "print('batch data: \\n\\n{}\\n \\nbatch labels: \\n\\n{}'.format(batch_data, batch_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_index = 0\n",
    "\n",
    "def generate_test_batch_words(data, batch_size, num_test_steps):\n",
    "    global test_data_index\n",
    "\n",
    "    batch = np.ndarray(shape=(batch_size,), dtype=np.int32)\n",
    "    \n",
    "    for bi in range(batch_size):\n",
    "        batch[bi] = data[test_data_index + num_test_steps]\n",
    "        test_data_index = (test_data_index + 1) % batch_size\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 128 \n",
    "window_size = 2\n",
    "num_sampled = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I/p and O/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_data = tf.placeholder(tf.int32, shape = [batch_size, 2 * window_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
    "\n",
    "test_labels = tf.placeholder(tf.int32, shape = [batch_size], name = 'test_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0, dtype = tf.float32))\n",
    "\n",
    "softmax_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev = 1.0 / math.sqrt(embedding_size)))\n",
    "softmax_biases = tf.Variable(tf.zeros([vocab_size]), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked embedding size: [64, 128, 4]\n",
      "Reduced mean embedding size: [64, 128]\n"
     ]
    }
   ],
   "source": [
    "mean_batch_embeddings = tf.reduce_mean(tf.nn.embedding_lookup(embeddings, test_labels), axis = 0)\n",
    "\n",
    "stacked_embeddings = None\n",
    "\n",
    "for i in range(2*window_size):\n",
    "    embedding_i = tf.nn.embedding_lookup(embeddings, train_data[:, i])\n",
    "    x_size, y_size = embedding_i.get_shape().as_list()\n",
    "    \n",
    "    if stacked_embeddings is None:\n",
    "        stacked_embeddings = tf.reshape(embedding_i, [x_size, y_size, 1])\n",
    "    else:\n",
    "        stacked_embeddings = tf.concat(axis = 2, values = [stacked_embeddings, tf.reshape(embedding_i, [x_size, y_size, 1])])\n",
    "\n",
    "        \n",
    "print(\"Stacked embedding size: %s\"%stacked_embeddings.get_shape().as_list())\n",
    "\n",
    "mean_embeddings = tf.reduce_mean(stacked_embeddings, axis = 2, keepdims = False)\n",
    "\n",
    "print(\"Reduced mean embedding size: %s\"%mean_embeddings.get_shape().as_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights = softmax_weights,\n",
    "                                                 biases = softmax_biases,\n",
    "                                                 inputs = mean_embeddings,\n",
    "                                                 labels = train_labels,\n",
    "                                                 num_sampled = num_sampled,\n",
    "                                                 num_classes = vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CBOW on Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 6.366329\n",
      "Average loss at step 200: 4.315928\n",
      "Average loss at step 400: 3.507949\n",
      "Average loss at step 600: 3.388779\n",
      "Average loss at step 800: 3.375359\n",
      "Average loss at step 1000: 3.106099\n",
      "Average loss at step 1200: 3.224767\n",
      "Average loss at step 1400: 3.210920\n",
      "Average loss at step 1600: 3.079953\n",
      "Average loss at step 1800: 2.954291\n",
      "Average loss at step 2000: 3.159761\n",
      "Average loss at step 2200: 2.822422\n",
      "Average loss at step 2400: 2.868108\n",
      "Average loss at step 2600: 3.189792\n",
      "Average loss at step 2800: 3.067338\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "cbow_loss = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        batch_data, batch_labels = next_batch(step, batch_size)\n",
    "    \n",
    "        feed_dict = {train_data: batch_data, train_labels: batch_labels}\n",
    "        \n",
    "        _, l = sess.run([optimizer, loss], feed_dict = feed_dict)\n",
    "        avg_loss += l\n",
    "        \n",
    "        if (step) % 200 == 0:\n",
    "            if step > 0:\n",
    "                avg_loss = avg_loss / 200\n",
    "            print('Average loss at step %d: %f' % (step, avg_loss))\n",
    "            \n",
    "            cbow_loss.append(avg_loss)\n",
    "    \n",
    "    # Computing document embeddings by averaging word embeddings\n",
    "    document_embeddings = {}\n",
    "    num_test_steps = 5\n",
    "    \n",
    "    for doc, words in test_doc_int.items():\n",
    "        \n",
    "        test_data_index = 0\n",
    "        topic_mean_batch_embeddings = np.empty((num_test_steps,embedding_size),dtype=np.float32)\n",
    "        \n",
    "        for step in range(num_test_steps):\n",
    "            test_batch_labels = generate_test_batch_words(test_doc_int[doc],batch_size, step)\n",
    "            batch_mean = sess.run(mean_batch_embeddings,feed_dict={test_labels:test_batch_labels})\n",
    "            \n",
    "            topic_mean_batch_embeddings[step, :] = batch_mean\n",
    "        document_embeddings[doc] = np.mean(topic_mean_batch_embeddings)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
